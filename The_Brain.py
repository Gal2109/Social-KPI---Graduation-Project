# -*- coding: utf-8 -*-
"""Final_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d-LG8KYciNGquw6hipzOtS_U6i-YfljR
"""

import numpy as np
import pandas as pd
# import pingouin as pg
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.model_selection import cross_val_score
from sklearn.impute import KNNImputer
from sklearn.metrics import classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import AdaBoostClassifier
from sklearn.datasets import make_classification
from sklearn.neighbors import KNeighborsClassifier
import warnings
warnings.filterwarnings("ignore")

df = pd.read_csv('big_table.csv')

df.head(5)

# USAGE - leaving out only domestic information (omit buisness and so on)
df = df.query('usage == 1')

df_origin = df


# ARNONA_CAT - arbitrar scoring
df.arnona_cat = pd.Categorical(df.arnona_cat)
df['arnona_cat_code'] = df.arnona_cat.cat.codes.astype(int)
# MARTIAL - arbitrar scoring
df.martial = pd.Categorical(df.martial)
df['martial_code'] = df.martial.cat.codes

df_origin

"""# **Catagorization by value**

In order to asses how good or bad is something we have to catarogize the value into a fixed scale, there are 3 kinds of parameters (a column that takes part in metric calculation) the we be treated diffrently

**Personal parameter**- An individual information, the value refers to the specific household

**(probability) aggregated parameter**  - This value was calculated by dividing the total cases with the total population by S.A

**Non-probability aggregated parameter** - This value's origin is given by the S.A but not as probability, for example socia-eco level (ranked 1 to 7)


* Null Treatment: Unexcpected values and Null values will be assigned as 0 so their effect on the metric will be cancled

#Catagorization by value (personal parameters)

For each parameter  we will define numeric value from -2 to 2. where -2 indicates the worst negative effect on social metric, 2 indicates the best positive effect and 0 indicates no effect (for example for null values)


Referred to Metrics:
non-Lonliness
health
economic strength
"""

# df['TEMPLATE_score'] = df['TEMPLATE'].apply(lambda x : -2 if x<=y_1  else (-1 if y_1<x<=y_2 else (1 if y_3<x<=y_4  else (2 if x > y_4 else 0 )))))  where y_1<...<y_4, else take care of any z s.t y_2<z<=3 OR any unexcpected values

arnona_cat_score_dict = { 'Construction' : 0, 'Empty' : 2, 'corona19' : -2 ,'warTrauma' : -2 ,'soldier' : -2 ,'other' : -2 ,'one_parent' : -2 ,'income' : -2 ,'immigrants' : -1 ,'elderlies_nursing' : -2 ,'elderlies' : -1 ,'disabilities' : -2 }
df['arnona_cat_score'] = df['arnona_cat'].apply(lambda x : arnona_cat_score_dict[x] if x==x else 0)
df['arnona_cat_score'] = df['arnona_cat_score'].fillna(0)

martial_score_dict = { 'אלמן/ה' : -2, 'גרוש/ה' : -2, 'נשוי/ה' : 2 ,'רווק/ה' : 0 }
df['martial_score'] = df['martial'].apply(lambda x : martial_score_dict[x] if x==x else 0)
df['martial_score'] = df['martial_score'].fillna(0)

df['members_Water_score'] = df['members_Water'].apply(lambda x : -2 if x==1  else (1 if 1<x<4 else (2 if x>3  else 0)))
df['age_score'] = df['age'].apply(lambda x : 2 if 18<x<44  else (-1 if 43<x<64 else (-2 if x>63  else 0)))

df.rename(columns = { 'near 106 pizul and dangerous buildings' : 'near_106_pizul_and_dangerous_buildings' }, inplace = True)
df['near_106_pizul_and_dangerous_buildings_score'] = df['near_106_pizul_and_dangerous_buildings'].apply(lambda x : -2 if x==2  else 0)

df["count"] = df.groupby("STAT")["index"].transform('count') # population size of a statistic area
# df.head()
df = df.iloc[: , 1:]
df.head()

# PROBABILITY FUNCTION - gets a numeric number and devide by the total population for that s.a (statistical area)
def prob_func(df, col_name):
  df[f'{col_name}'] = df[f'{col_name}']/df['count']

prob_cols = ['widow_grown', 'widow_elderlies', 'lonely_elderlies', 'p85_plus', 'avtachat_hachansa_family', 'mekabley_kizva_elderlies', 'hashlamta_hachnasa_family_eldelies', 'hashlama_kizvat_nechut_elderlies', 'Hashlamat_hachnasa_sheerim_family', 'Mekabley_mezonot', 'Mekabley_kizbaot_nechut', 'zachaim_kizbat_nechut_children', 'mekabley_kizbaot_from_injured_Work', 'mekabley_kizba_siud', 'accumulated_cases', 'accumulated_recoveries', 'accumulated_hospitalized', 'accumulated_vaccination_first_dose', 'accumulated_vaccination_second_dose', 'accumulated_vaccination_third_dose']
# probcols is the list of all aggrigated parameters, it shows the probability of one to be *col name*
for col_name in prob_cols:
  prob_func(df, col_name)

# INCOME COLUMNS
df['total_income'] = df['all_jobs_no_household_jobs_mean']+df['household_jobs_mean']+df['pension_mean']+df['kizbat_sheerim_mean']+df['self_employed_mean']
df['income_per_person'] = df['total_income']/(df['members_Water']+1)

"""#Catagorization by value (Prob. Aggregated parameters)

Aggregated parameters will be calculated b exceptation-effect, their effect will be lower (aside from the given weights) than individual parameters mentioned above.
the effect of probability parameter is going to effect as following:

effect = probability (the value held by the col) * (-2) (the negative effect by our scale) * W (weight 

for example the probability to be injured elder is 0.3, so the effect from this parameter will be 0.3*(-2)*W = -0.6 * the given weight
"""

for prob_col in prob_cols:
  df[f'{prob_col}_score'] = df[f'{prob_col}']*(-2)
  df[f'{prob_col}_score'] = df[f'{prob_col}_score'].fillna(0)

"""##Catagorization by value (Non-probability Aggregated parameters):"""

df['socio_economic'].describe()

df['socio_economic'].hist()

# df['income_per_person'].quantile(0.25)
q25 = df['income_per_person'].quantile(0.25)
q50 = df['income_per_person'].quantile(0.5)
q75 = df['income_per_person'].quantile(0.75)
df['income_per_person_score'] = df['income_per_person'].apply(lambda x : -2 if x<=q25  else (-1 if q25<x<q50 else (1 if q50<x<q75 else(2 if x>=q75 else 0))))

df['socio_economic_score'] = df['socio_economic'].apply(lambda x : -2 if 1<=x<=2  else (-1 if x==3 else (1 if x==5 else(2 if 6<=x<=7 else 0))))

"""Focusing the dataframe"""

all_scored_params = [] 
for col in df.columns:
  if ('score' in col):
    all_scored_params.append(col)

all_scored_params # hold all of the parameters (which may effect the metrics)

df

#Lets make the DF more readable and relevant by leaving only the are (stat and coordiantes):

df_scores = df[['index','STAT','north','east']+all_scored_params]
df_scores

pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)

def update_weights(metric_dict, param_to_update, new_weight): # updated given dict with a new value
  up_dict = {f'{param_to_update}' : new_weight}
  updated_metric_dict = metric_dict.update(up_dict)

  if round(sum(metric_dict.values()), 4) != 1:
    print(f"WARNING: dictionary weights are not summed to 1")
    print("sum:", round(sum(metric_dict.values()), 4) )
  else:
    print(f"---Dictionary sums to 1---")
  if param_to_update not in metric_dict:
    print("WARNING: param does not exist in dictionary")

  return updated_metric_dict

loneliness_dict = {} #recall loneliness is non-lonliness in fact
health_dict = {}
economic_strength_dict = {}


for col in df_scores.columns:
  loneliness_dict[col] = 0
  health_dict[col] = 0
  economic_strength_dict[col] = 0

#Default Weights Initiate

update_weights(loneliness_dict, 'arnona_cat_score', 0.1 )
update_weights(loneliness_dict, 'members_Water_score', 0.08 )
update_weights(loneliness_dict, 'martial_score', 0.1 )
update_weights(loneliness_dict, 'widow_grown_score', 0.04 )
update_weights(loneliness_dict, 'widow_elderlies_score', 0.1 )
update_weights(loneliness_dict, 'lonely_elderlies_score', 0.25 )
update_weights(loneliness_dict, 'p85_plus_score', 0.06 )
update_weights(loneliness_dict, 'accumulated_cases_score', 0.05)
update_weights(loneliness_dict, 'age_score', 0.1 )
update_weights(loneliness_dict, 'area_per_person_score', 0.07 )
update_weights(loneliness_dict, 'Ownership_score', 0.05)

update_weights(health_dict, 'arnona_cat_score', 0.2 )
update_weights(health_dict, 'age_score', 0.08 )
update_weights(health_dict, 'hashlama_kizvat_nechut_elderlies_score', 0.08 )
update_weights(health_dict, 'Mekabley_kizbaot_nechut_score', 0.1 )
update_weights(health_dict, 'zachaim_kizbat_nechut_children_score', 0.09 )
update_weights(health_dict, 'mekabley_kizbaot_from_injured_Work_score', 0.11 )
update_weights(health_dict, 'mekabley_kizba_siud_score', 0.15 )
update_weights(health_dict, 'accumulated_cases_score', 0.05 )
update_weights(health_dict, 'accumulated_recoveries_score', 0.01 )
update_weights(health_dict, 'accumulated_hospitalized_score', 0.07 )
update_weights(health_dict, 'accumulated_vaccination_first_dose_score', 0.02 )
update_weights(health_dict, 'accumulated_vaccination_second_dose_score', 0.02 )
update_weights(health_dict, 'accumulated_vaccination_third_dose_score', 0.02 )

update_weights(economic_strength_dict, 'area_per_person_score', 0.03 )
update_weights(economic_strength_dict, 'socio_economic_score', 0.05 )
update_weights(economic_strength_dict, 'mekabley_kizba_siud_score', 0.02 )
update_weights(economic_strength_dict, 'mekabley_kizbaot_from_injured_Work_score', 0.02 )
update_weights(economic_strength_dict, 'zachaim_kizbat_nechut_children_score', 0.02 )
update_weights(economic_strength_dict, 'Mekabley_kizbaot_nechut_score', 0.02 )
update_weights(economic_strength_dict, 'Mekabley_mezonot_score', 0.02 )
update_weights(economic_strength_dict, 'Hashlamat_hachnasa_sheerim_family_score', 0.02 )
update_weights(economic_strength_dict, 'hashlama_kizvat_nechut_elderlies_score', 0.02 )
update_weights(economic_strength_dict, 'hashlamta_hachnasa_family_eldelies_score', 0.02 )
update_weights(economic_strength_dict, 'mekabley_kizva_elderlies_score', 0.02 )
update_weights(economic_strength_dict, 'avtachat_hachansa_family_score', 0.02 )
update_weights(economic_strength_dict, 'income_per_person_score', 0.2 )
update_weights(economic_strength_dict, 'arnona_cat_score', 0.1 )
update_weights(economic_strength_dict, 'Ownership_score', 0.20 )
update_weights(economic_strength_dict, 'age_score', 0.05 )
update_weights(economic_strength_dict, 'martial_score', 0.02 )
update_weights(economic_strength_dict, 'members_Water_score', 0.05 )
update_weights(economic_strength_dict, 'near_106_pizul_and_dangerous_buildings_score', 0.1 )

"""# Dynamic Updating

The main goal is to update the DF by the new wights given by the GUI.
GUI will provide a tuple consist of the following tuple: (metric_type, dictionary), where metric_type is a string of "H" or "L" or "E" for Health, Loneliness and Economic strength accordingly and the dictionary is the actual dictionary holding the new weights data in the same format we have used so far.The next fuction will update the DF by this
"""

mapping_dict = {"E" : economic_strength_dict, "H" : health_dict, "L" : loneliness_dict } #maps from a letter to the corresponding dictionary
def weights_update(GUI_tuple):
  metric_str = GUI_tuple[0]
  curr_dict = mapping_dict[metric_str] # holds the dict we want to update
  new_weights_dict = GUI_tuple[1] # holds the dict with the new weights

  for param, weight in new_weights_dict.items():
    update_weights(curr_dict, param, weight)

# health_dict

#DYNAMIC UPDATE TEST

# updated_H_dict_example = {"hashlama_kizvat_nechut_elderlies_score" : 0.0, "arnona_cat_score" : 0.3}
# gui_tup_example = ("H", updated_H_dict_example)
# health_dict

"""## Metrics Creation

In this stage we create new column for each metric (Loneliness, Health and Eco strength) where the we sum the weighted values from the parameters. 

for example

Loneliness metric for the i_th row is defined as:

L_i = param_1 * w_1 + ... + param_n * w_n


"""

pd.DataFrame([loneliness_dict])

columns_list = df_scores.columns
df_scores['Loneliness'] = 6 - df_scores.apply(lambda row: sum([row[col] *loneliness_dict[col] for col in columns_list]), axis=1) # Now loneliness is not non-loneliness anymore (5 = lonenly)
df_scores['Health'] = df_scores.apply(lambda row: sum([row[col] *health_dict[col] for col in columns_list]), axis=1)
df_scores['Economic_Strength'] = df_scores.apply(lambda row: sum([row[col] *economic_strength_dict[col] for col in columns_list]), axis=1)

df_scores

df_scores['Loneliness'].hist()
plt.show()
df_scores['Health'].hist()
plt.show()
df_scores['Economic_Strength'].hist()

"""Normalizing Metrics Scores into 1 to 5 scale"""

Metrics = ['Loneliness', 'Health', 'Economic_Strength']
for metric in Metrics: 
  q20 = df_scores[metric].quantile(0.20)
  q40 = df_scores[metric].quantile(0.40)
  q60 = df_scores[metric].quantile(0.60)
  q80 = df_scores[metric].quantile(0.80)
  q100 = df_scores[metric].quantile(0.100)
  df_scores[f'{metric}_score'] = df_scores[metric].apply(lambda x : 1 if x<=q20  else (2 if q20<x<=q40 else (3 if q40<x<=q60 else(4 if q60<x<=q80 else 5))))

"""# Risk Metric

The following metric is the most general (binary) metric which indicates whether a treatment or inspection by the authority is required for a household.
 

Risk is calculated as a function of the previous metrics as following:


**Denote metrics:**

H = healh metric

E = economic strength

L = lonelienss

NL = 6 - L = non-loneliness (in order to normalize with other metrics)


**Risk domain:**

(1,1,1) < Risk < (5,5,5)

**Risk function:**

K = 5 (constant for mathematical simplification)

In order to emphasize the effect of low scores (such as 1 L or 2 H) we will power the values, the function of risk is the sum of squares.

**R(NL,H,E) = (K-NL)^2 + (K-H)^2 + (K-E)^2**

**Risk Threshold:**

Now let's define a threshold T such that for the ith household:

If R(NL,H,E)_i > T: household i is under risk,

Otherwise, the household isn't under risk.
"""

NL = df_scores['Loneliness_score']
E = df_scores['Economic_Strength_score']
H = df_scores['Health_score']
K_const = 5
T_threshold = 34 # risker than (1,2,2)

df_scores['R_function'] = np.power(K_const-NL,2) + np.power(K_const-H,2) + np.power(K_const-E,2)
df_scores['Risk'] = df_scores['R_function'].apply(lambda x : 1 if x>= T_threshold else 0)

print(round(df_scores.query('Risk == 1').count()[1]/df_scores.shape[0]*100 ,3), '% of the households are under risk')

"""# Creating sub-df for different proposes

KNN-DF - includes the raw data with the calculated metrics

"""

#This DF is the raw data (queried on usage==1) with the given scores (metrics). Good for classification puposes
df_origin_plus_scores = pd.merge(df_origin,df_scores[['index','Loneliness_score',	'Health_score',	'Economic_Strength_score']],on = 'index', how = 'left') 

#This DF is the raw data (queried on usage==1), without S.A parameters with the given scores (metrics). Good for WizSoft reports
df_orig_no_SA_plus_scores = df_origin_plus_scores[['index','STAT','heb_address','Ownership','usage','arnona_cat','east','north','members_Water','age','martial' , 'mevunehArea','area_per_person','near abandon houses','near cellular antenas','near 106 bitachon reports','near_106_pizul_and_dangerous_buildings','possilbe buildings with pizul','possible pizul within building','deals_count_x','price_change_ratio_x','Gush','Helka','gush_helka','Loneliness_score',	'Health_score',	'Economic_Strength_score']]

#This DF includes location, raw data, and final metrics (ncluding Risk)
df_raw_scores_risk = pd.merge(df_origin,df_scores[['index','Loneliness_score',	'Health_score',	'Economic_Strength_score','Risk']],on = 'index', how = 'left')

df_raw_scores_risk.tail(15)

from google.colab import files
df_orig_no_SA_plus_scores.to_csv('df_orig_no_SA_plus_scores.csv', index=False ,encoding = 'utf-8-sig')
# files.download('df_orig_no_SA_plus_scores.csv')

df_raw_scores_risk.to_csv('df_raw_scores_risk.csv', index=False ,encoding = 'utf-8-sig')
# files.download('df_raw_scores_risk.csv')

df_knn_raw = df_origin_plus_scores

"""# Risk Prediction

Given a new household data (row), we want to predict whether it is under risk or not. 
The prediction we be based on the labeled data and not by calculating metrics as done before.

Methodology:

1. Refer to df_scores as out training data, s.t:
  > X = x_1,...,x_n = param_1,...,param_n

  > Y = y_1, y_2, y_3 = N, H, E

  > Y classes = {1,...,5}
2. Train KNN model based on labeled df_scores
3. Predict for the new row each y in Y separately (run 3 machines)
4. Calculate risk by our risk function





"""

#Transform strings to numeric
df_knn = df_origin_plus_scores

# ARNONA_CAT - arbitrar scoring
df_knn.arnona_cat = pd.Categorical(df_knn.arnona_cat)
df_knn['arnona_cat'] = df_knn.arnona_cat.cat.codes.astype(int)
# MARTIAL - arbitrar scoring
df_knn.martial = pd.Categorical(df_knn.martial)
df_knn['martial'] = df_knn.martial.cat.codes
# Predominant Leum - arbitrar scoring
# df_knn['Predominant_Leum'] = df_knn['Predominant Leum']
# df_knn.drop(columns=['Predominant Leum'], inplace = True)
df_knn.rename(columns = { 'Predominant Leum' : 'Predominant_Leum' }, inplace = True)

df_knn.Predominant_Leum = pd.Categorical(df_knn.Predominant_Leum)
df_knn['Predominant_Leum'] = df_knn.Predominant_Leum.cat.codes
# Predominant_enter_exit_group - arbitrar scoring
df_knn.Predominant_enter_exit_group = pd.Categorical(df_knn.Predominant_enter_exit_group)
df_knn['Predominant_enter_exit_group'] = df_knn.Predominant_enter_exit_group.cat.codes

#ADRESS - omitting (adress information is given anyway by coordinates)
df_knn.drop(columns=['heb_address'], inplace = True)
df_knn

df_knn.dropna(axis = 1, inplace = True) # remove NA

df_knn = df_knn[np.isfinite(df_knn).all(1)] #remove inf

# pd.set_option('display.max_rows', 500)

from sklearn.model_selection import train_test_split

df_knn_train, df_knn_test = train_test_split(df_knn, test_size=0.2)

X_train = df_knn_train.iloc[: , :-1]
y_train = df_knn_train.iloc[: , -1:]

X_test = df_knn_test.iloc[: , :-1]
y_test = df_knn_test.iloc[: , -1:]

list_of_mean = []
range_list = []
for n in range(1,16):
    classifier = KNeighborsClassifier(n_neighbors=n)
    cv_scores = cross_val_score(classifier, X_train, y_train, cv=5)
    list_of_mean.append(cv_scores.mean())
    range_list.append(n)
    # print(cv_scores, "\n")

plt.plot(range_list, list_of_mean)
plt.xlabel("K values")
plt.ylabel("Avg")
plt.show()

neigh = 3

classifier = KNeighborsClassifier(n_neighbors=neigh)
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)


print(round(metrics.accuracy_score(y_test, y_pred)*100,2) , '% Accuracy')

